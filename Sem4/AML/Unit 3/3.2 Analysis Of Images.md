---
alias: 
tags: aml unit3
---

## PCA

Principle component analysis of images.
Dim reduction technique. To extract meaningful data in a high dimensional image data. 

### Steps to apply PCA

**Image Representation:** 
1) Represent image in Multi dimensional arrays, each element representing pixel intensity and color value.
2) 2D array for Grayscale and 3D for color channels.

**Data Preprocessing**
1) Preprocessing involves ensuring 0 mean and unit(1) variance.
	Helps in normalizing data to ensure all features are comparable.

**Flattening of Image**
1) Flatten it to a 1D array. Irrespective of color channels. An greyscale image of 100x100 gets transformed to vector of length 10,000.

**Covariance Matrix Calc**
1) Flattened data is used for COV.
2) Relationship between pair of image features is found.

**Edge Decomposition**
1) COV matrix is decomp to eigen vectors and eigen values. 
2) Eigen vectors represent PC. Indicates the directions of max var in data.
3) Eigen Values indicate amt. of var explained by each eigenvector.

**Dimensionality Reduction**
1) Eigen Vectors are ranked on basis of corresponding eigenvalues.
2) Top k eigenvectors (highest variance) are selected as PC. 
3) These PC form a new basis for representing the image in lower dim space.


## Denoising Technique

Removal of noise(unwanted variations) form images which can degrade quality and clarity of image. Ways to clear noise can be traditional or deep learning based.

Traditional Denoising Technique: **Denoising Filters**

### Gaussian Filter
- Averages the pixel values in the neighborhood reducing the impact of noise. 
- It's a low pass Linear filter, reducing high freq. noise while preserving edges and details. 
- Uses a gaussian kernel to process image, which assigns weights based on gaussian distribution. 
- Smoothing applied is determined by the size of kernel.

### Median Filter
- Replaces pixel value with median value of its neighboring pixels.
- Its a non linear filter. Good for removing salt and pepper noise, occurs as randomly occurring dark and bright pixels. 
- Preserves edges and details better than Gaussian Filter.

Example of Denoising:
```python
import cv2 
# Load the noisy image 
image = cv2.imread('noisy_image.jpg', cv2.IMREAD_COLOR) 
# Apply median filter for denoising 
denoised_image = cv2.medianBlur(image, 5) #Kernel size 5x5
# Display the original and denoised images 
cv2.imshow('Original Image', image) 
cv2.imshow('Denoised Image', denoised_image) 
cv2.waitKey(0) #Watiing for key press before next line
cv2.destroyAllWindows() #Closes the window
```

## Haris Corner Detector

Used for detecting corner or interest points in an image. Calc. corner response function using image gradients and then selecting the point with high corner responses as corner.

**Steps:**
1) Convert the image to grayscale. Done for simpler computations.
2) Compute image gradients. Calc. is done in horizontal and vertical directions. Uses derivative filters to determine the intensity changes across the images. 
3) Harris matrix is calculated, its elements are calculated by squaring and multiplying gradients. It captures the local image structure. 
4) Apply Gaussian Smoothing, Gaussian filter is used to denoise to improve detection. 
5) Corner response function is calculated, it determines the likelihood of a pixel being a corner. 
6) Threshold for CRF. Only pixels above certain CRF score can be corner candidates. 
7) Non maximum suppression, Removes redundant corner candidates by considering only pixels with max corner response in their local neighborhood. 
8) Display Corner, in a visual way or give coordinates of the corner.


## SIFT - Scale Invariant Feature Transform

Widely used vision algorithm.

**Steps:** 

1. Scale-space extrema detection: SIFT constructs a scale-space pyramid to detect features that are invariant to scale changes. It identifies potential interest points as local extrema in the Difference of Gaussians (DoG) scale-space.
    
2. Keypoint localization: Detected extrema are refined to locate precise keypoints. A quadratic function is fitted to nearby points in the scale-space to estimate the location and scale of keypoints. Low-contrast or poorly localized keypoints are discarded.
    
3. Orientation assignment: SIFT assigns an orientation to each keypoint for invariance to image rotation. It computes gradient magnitude and orientation in the neighborhood of the keypoint and selects the dominant orientation as the keypoint's orientation.
    
4. Keypoint descriptor generation: SIFT constructs a descriptor for each keypoint to capture its distinctive characteristics. It samples a local image patch around the keypoint and computes histograms of gradient orientations and magnitudes. The histograms are concatenated to form the keypoint descriptor.
    
5. Keypoint matching: SIFT features can be matched across different images by comparing their descriptors. Nearest neighbor matching with a distance metric, such as Euclidean distance or cosine similarity, is commonly used. RANSAC can be used to further refine matches and estimate geometric transformations between images.



## Matching Geo Tagged Images

Finding correspondences between images as their geo location metadata(Latitude, longitude). 

Useful for image retrieval, 3d reconstuction and panoroma.

1. First, associate image with geo location. Using GPS devices or location metadata in digital images.
    
2. Feature extraction: Distinctive features are extracted from the images using algorithms like SIFT or SURF.
    
3. Geographic similarity: A measure is used to compare the features and geographic info to find similar images.
    
4. Nearest neighbor matching: Each image's features are compared with others to find the most similar matches.
    
5. Geometric verification: Techniques like RANSAC are used to confirm and refine the matches, removing outliers.
    
6. Filtering: Additional checks and algorithms are applied to ensure accurate matching between images.


Ex: 
Imagine you have a travel app where users can upload photos from their trips, and these photos are geotagged with their location information (latitude and longitude). The app allows users to search for similar photos from other travelers based on their current location.

Here's how the process works:

1. Geotagging: When a user takes a photo using the app's camera or uploads a photo from their device, the app automatically adds the geolocation metadata to the image.
    
2. Feature extraction: The app uses feature extraction algorithms like SIFT or SURF to extract key features from the geotagged images. These features could include distinct landmarks, buildings, or natural features.
    
3. Geographic similarity: The app compares the features and geographic information of the user's photo with other photos in its database.
    
4. Nearest neighbor matching: The app finds the most similar photos from other users who have visited the same or nearby locations. It uses nearest neighbor matching to find the closest matches.
    
5. Geometric verification: To ensure accurate matching, the app applies geometric verification techniques like RANSAC to eliminate any mismatches or outliers.
    
6. Filtering and display: The app filters the matched photos and displays them to the user, showing images from other travelers who have visited similar places. Users can then explore these photos to get inspiration for their own travels or to connect with other travelers who have been to similar locations.