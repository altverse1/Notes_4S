
## Custom Transformers

Scikit learn allows the creation of Custom Transformers.
Transforms data from one representation to another. Learns from training data and applies learnt method of transformation to the new data.

Basically it's user defined transformation.

To create Custom transformer we have to create our own class with fit and transform methods.

```python
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import StandardScaler

class StandardizeTransformer(BaseEstimator, TransformerMixin):
	def _init_(self):
		self.scaler = StandardScaler()
	def fit(self,X,y=None)
		self.scaler.fit(X)
		return self
	def transform(self,X,y=None)
		return self.scaler.transform(X)
```

`StandardizeTransformer` inherits from `BaseEstimator` and `TransformerMixin`
- `BaseEstimator` - Provides implementations of methods such as `get_params`, `set_params`.  
- `TransformerMixin` - Provides implementation of `fit_transform` method aka fit and transform methods.

In `StandardizeTransformer` , method `fit` fits a `StandardScaler` object to the training data X(learning) & `transform` applies the learned scaling to new data X.

We can use this with pipeline. `pipeline = make_pipeline(StandardizeTransformer(), LinearRegression())` and then `pipeline.fit(X_train, y_train)`. Here the pipeline is of a custom transformer and a machine learning model. 


## Feature Scaling 

Its a transformation method.

### Min Max Scaling / Normalization
Is used for transforming features of dataset into a common range. Data is rescaled to a range between 0 to 1. This way features will have a similar scale and won't affect the algorithm in a drastic way. 

$$ X_{scaled} = \frac{x-x_{min}}{x_{max}-x_{min}} $$

x_min is the Minimum value of the feature
x_max is the Maximum value of the feature

This method is useful when:
- Distribution of the feature is skewed. 
- When range of value is different from that of other features.

Improves the performance of algorithms which are sensitive to the scale of input features. Such as K-NN and SVM

```python
from sklearn.preprocessing import MinMaxScaler
x = np.array([[1,2,3],[4,5,6],[7,8,9]])
scaler = MinMaxScaler()
X_Scaled = scaler.fit_transform(x)
print(X_Scaled)
```

```
Output: 
[[0.  0.  0. ]
 [0.5 0.5 0.5]
 [1.  1.  1. ]]

```

### Standardization/Z-Score Normalization/ Standard Scaling

This aims to transform the features such that they have 0 mean and Unit variance.

$$ X_{scaled} = \frac{x-mean} {standard \,   deviation}
$$

x: the original feature value
mean: Mean of the features
Standard_deviation: s.d of the features

```python
from sklearn.preprocessing import StandardScaler

df = pd.read_csv('your_data.csv')
feature_col = [col1, col2, col3] #feature col. to scale
scaler = StandardScaler()
df[feature_col] = scaler.fit_transform(df[feature_col])
```

>Links
[[1.1.2.1 Pipelines]]
