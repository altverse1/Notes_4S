Non numeric attributes are equally useful as their numeric counterparts but cant be used in directly in mathematical models. That's why embedding is used for handling text and categorical attributes. 


## Label Encoding

This method converts each unique category into numeric value. Ex. For attribute `color` with values `red`, `green`, `blue`  label encoding will convert the values into 2,1,0

```python
from sklearn.preprocessing import LabelEncoder
data = ['red','green','blue','green','red']
le = LabelEncoder()
encode_data = le.fit_transform(data)
ecode_data
```
	Output: [2,1,0,1,2]

## One-Hot Encoding 

This method converts each category into a binary vector where presence of the element is indicated by 1 and absence as 0. Taking the same attribute color, with one hot encoder we get three new rows red, green and blue.

```python
import pandas as pd
from sklearn.preprocessing import OneHotEncoder

df = pd.DataFrame({'color':['red','green','blue','green','red']})
ohe = OneHotEncoder()

encode_data = ohe.fit_transform(df['color'])

encoded = pd.DataFrame(encode_data.toarray())
```

## Word Embedding
Text data -> Numerical Data
Convert words into dense vectors that have a language related meaning. Ex: if we have data with column called `Product Reviews`, each review gets converted into numerical vector. 

```python
import numpy as np
import gensim

sentences = [['red'],['green'],['blue'],['red'],['green']]
model = gensim.models.Word2Vec(setences, min_count = 1, size = 100, window = 5, sg = 1)
word_embeddings = []

for sentence in sentences:
	for word in sentence:
		word_embeddings.append(model.wv[word])

word_embeddings = np.array(word_embeddings)
print(word_embeddings)
```
