Capable of linear and non linear classification(small to medium dataset), regression, and outlier detection. 

Many datasets are not linearly separable. 
- To handle non linear datasets is to add more features. Sometimes adding polynomial features results in linearly separable data. 
![[Pasted image 20230530194404.png]]
> Fig Adding features makes linearly scrapeable data.

Say we have x1 = (-4,-3,-2,-1,0,1,2,3,4)
Where Blue = (-4,-3,3,4) and rest are green. Currently we cannot separate them. But if we add another dimension say x2 = x1^2 and make it (x1,x2) we can plot and see the parabola which can seperate.
x2 = (4,3,2,1,0,1,2,3,4)
and the new blue = ((-4,4),(-3,3),(3,3),(4,4)) and rest green

- Main disadvantage is the curse of dimensionality which causes the model to overfit. 

Other types of polynomials can be xy,y^2 etc... 

Implementation: 

```python
from sklearn.datasets import make_moons
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures

polynomial_svm_clf = Pipeline([
	("poly features", PolynomialFeatures(degree = 3))
	("scaler",StandardScaler()),
	("svm_clf",LinearSVC(c=10, loss="hinge"))
])
polynomial_svm_clf.fit(x,y)
```

![[Pasted image 20230530195412.png]]
> Fig Linear SVM classifier using polynomial features


## Handling Non Linear Data using Kernel Functions

- Transform data into a higher dimensional feature space enabling algorithms to work on non linear classifications.  
- Kernel Functions allow us to get same result of polynomial features without adding new features. It does so by using various math techniques.
- Kernel functions use a similarity measure between pairs of data points, does so by calculating inner product in the original feature space or a transformed feature space.


## Kernel Types:

### Polynomial Kernel

Using technique called kernel trick, we get similar results as to adding polynomial features without having to add. 

```python
from sklearn.svm import SVC
poly_kernel_svm_clf = Pipeline([
	("scaler",StandardScaler()),
	('svm_clf', SVC(kernel="poly", degree = 3, coef0 = 1, C=5))
])
poly_kernel_svm_clf.fit(x,y)
```

Poly Kernel and it's function:
- The polynomial kernel function is defined as K(x, y) = (γ * + r)^d, where x and y represent the input feature vectors, denotes the dot product between them, and d is the degree of the polynomial.
- The dot product <x,y> captures the similarity between the feature vectors in the original feature space.
- By raising the result of the dot product to the power of d and scaling it by the parameter γ, the polynomial kernel computes the similarity between two data points in a higher-dimensional space.


### RBF Kernel aka Gaussian RBF Kernel Function

Uses Euclidian distance, measures how close two data points are to each other in a high dimensional data space.

RBF Kernel and it's function:
- The RBF kernel function is defined as K(x, y) = exp(-γ * ||x - y||^2), where x and y represent the input feature vectors, ||x - y|| denotes the Euclidean distance between them, and γ is a hyperparameter that controls the spread of the kernel.
- The RBF kernel computes the similarity between two data points based on the exponential of the negative squared distance between them.

```python
from sklearn.svm import SVC
rbf_kernel_svm_clf = Pipeline([
	("scaler",StandardScaler()),
	('svm_clf', SVC(kernel="rbf", gamma=5, C=0.001))
])
rbf_kernel_svm_clf.fit(x,y)
```

## SVM for regression

Builds regression model by finding hyperplane that fits the training data.
Does this by minimizing the distance between the positive and negative hyperplane(epsilon) to get a hyperplane that acts as best fit.

```python
from sklearn.svm import LinearSVR
svm_reg = LinearSVR(epsilon = 1.5)
svm_reg.fit(x,y)
```

![[Pasted image 20230530203501.png]]

> What is hyperplane?
> Decision boundary that separates different classes in feature space. For classification SVM tries to create hyperplane with max margin. 