Aka. Logit Regression, is a **binary classification algorithm**.
Used for estimating Probablity that an instance belongs to a particular class.

ex. Wether a creature is a cat or dog. Estimated probability >50% then model predicts instance for such class. (Labels: Positive class '1' and Negative class '0'). This is a binary classifier. 

Logistic Regression Model computes weighted sum of input features(plus bias term).
This outputs the logistic(output is interpretable as Probablity) of this result. 
The logistic function: 

$$ \sigma(t) = \frac {1}{1+e^{-t}} $$

![[Pasted image 20230529213109.png]]
>Fig: Logistic Function


## Training & Cost Function

Training involves finding values of the model's parameter that minimize the cost function.

Cost function? It is the measure of error between a model's prediction and actual values.

We use binary cross entropy in logistic regression. It's the difference between predicted Probablity and actual labels(outcomes, like spam or not spam).

Finding the values for the parameters of the model that reduce the cost function is the primary goal. This process is carried out using an iterative algorithm such as gradient descent.

### Main components of Training process:

#### Hypothesis Function
Models the probability of positive class. Uses logistic function to bring the output value between 0 and 1, representing a probabilistic value. 
$$
h\theta(x)=\frac{1}{1+e^{-\theta^{\,T*x}}}
$$
hθ(x) is the predicted probability of positive class.  θ vector of model parameters. x is the vector of input features.


#### Cost Function
Measure of error between predicted probabilities and actual binary labels in the training data.  Logistic Loss(binary cross-entropy) function is the most used one of Logistic Regression. 
Penalizes the model for incorrect predictions, encourages it to adjust the parameters to minimize loss. 

For a single training example(x,y) y being the binary label, logistic loss function is:
$$
C(\theta)=-y*\log(h\theta(x))-(1-y)*\log(1-h\theta(x))
$$



## Decision Boundaries in Logistic Regression

It's a line/surface separating different classes in the input feature space. 
As a binary classification algorithm, division occurs into two regions/two classes. 

Decision boundary can be linear and non linear too. 

Optimization is the technique to determine the decision boundaries. It finds the minimum and maximum value of a given function. 


![[Pasted image 20230529231303.png]]


# SoftMax Regression

aka multinomial Logistic Regression
Supports multiclass classification. No need of combination of binary classifiers. 
Aim is to classify input into one of k class(k>2). 
Output: Probablity Distribution giving likelihood of input over K classes.

Method: 
SoftMax scores (aka logits) k(x) are calculated for each class for the instance k. 
$$s_k(x) = x^T\theta^{(k)}$$
Estimates probability pk of that instance belongs to class k by applying Soft Max function to the scores.
Scores are normalized. 

$$ \hat{p}_k = \sigma(s(x))_k = \frac{exp(s_k(x))}{\sum_{j=1}^{K}exp(s_j(x))}$$
- K is no. of classes
- s(x) scores of each class of instance x
- sigma(s(x))k is estimated probability that instance belongs to class k.

>Links
		[[1.2.2 SVM]]