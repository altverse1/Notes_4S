## Feature Selection

Selecting subset of relevant features from a larger feature set. The aim is to find the subset of most useful features that help the model the most.

Type of Feature Selection: 

- **Filter Method**
	- Independent of machine learning models
	- They eval the characteristics of feature by comparing them with target variable and rank them accordingly. 
	- A threshold is used to determine which features to select.
	- Common Method: chi square test, information gain.
	- Computationally efficient but can overlook feature interactions. 

- **Wrapper Method**
	- Iterative process of choosing best features for a specific (chosen) model. 
	- Common Method: forward selection, backward and recursive elimination.
	- Computationally Expensive but will not leave feature interactions.


- **Embedded Method**
	- Feature selection is a part of model selection. 
	- Incorporate feature selection during the training phase.
	- Regularization techniques like L1 and L2  Regularization is used.
	- Penalization and elimination irrelevant features is done during train.
	- Computationally efficient and provides auto feature selection.


# Math Related

## Mutual Information
Based on information gain and takes into account how features work together. MI is used to find dependencies between features and classes. The MI between a feature 't' and class 'i'
measures how much contribution the absence of a feature makes to classifying correctly to class 'i'.

Computed using:
F1:
![[aml_miform2.png]]

F2:
![[aml_miform1.png]]
In the filter approach, a filter is used to discard features having a low value of MI. We can also use the backward filter which discards features if its value of MI with the class is less than some with probability p. The forward filter also can be used which includes a feature if the MI is greater than with a probability p.


## Goodman Kruskal Method
aka Lambda Coefficient. Its a statistical measure to assess relationship between feature and class of categorical data. 
Used for predicting the predictive power of features for classifying.

$$ \lambda i = \frac{max(ni+, ni-) - max(n+, n-)}{n - max(n+, n-)}$$
- $V_i$ is defined as set of particular feature set can take.
	ex: Feature 'color' can have $V_i$ = {'green', 'red', 'blue'}
 - ni+ -> Count of observations of feature i with value Vi and belong to class +
 - ni- -> Count of observations of feature i with value Vi and belong to class -
 - n -> Total number of observations in dataset regardless of class.
 - n+ -> All instances belonging to + class
 - n- -> All instances belonging to - class
 - For individual features it uses the above formula to check for predictive power.

While calculating we asses the association between value of Vi and class variable. This helps us identify the values that contribute the most to the prediction.


## Laplace Score 

Assess the relevance of features based on graph Laplacian matrix. Used for higher dimensional data.

The Laplacian Score algorithm is used to select important features from a dataset. Here's how it works:

1. Laplacian Matrix: Represent the dataset as a matrix. The Laplacian matrix (L) is created to show relationships between samples.
    
2. Graph Representation: Imagine the data as nodes in a graph. Edges connecting nodes reflect sample similarity. Graph Laplacian derives from this, capturing data structure.
    
3. Laplacian Score Matrix: Calculate Laplacian Score using $f(T) * L * f$. f(T) is feature matrix transposed, L is Laplacian matrix. This finds feature importance, giving an m x m score matrix.
    
4. Feature Ranking: Higher scores mean more important features. Rank features based on scores, helping select significant ones.
    

In a nutshell, the Laplacian Score algorithm helps choose meaningful features by analyzing data relationships through a graph-based approach.



# Algorithmic

## Hybrid Genetic Algorithm

Using the principles of Genetic Algorithm we select feature subset that maximizes classification performance while minimizing computational complexity.

- Initialization: Select a group of potential feature subsets. Create a initial group of these subsets randomly. 
- Fitness Evaluation: Using accuracy, error rate evaluate how well each subset performs. Train and test a classifier using selected features the measure their effectiveness.
- Genetic Operations:
	- Selection: Choose best performing features, i.e. features with greater fitness values. 
	- Crossover: Combine selected features by exchanging/merging features to create new offspring.
	- Mutation: Introduce random but small changes to features. Do this to explore possibilities.
- Search Using Other Techniques: Introduce additional mechanisms to enhance search process. Improve feature selection process by using methods such as information gain, wrapper methods alongside Genetic Algorithm.
- Terminate: A stopping criteria like number of generations or not seeing any further improvements over m iterations.

After termination select the best individual or subset on the fitness value obtained during evaluation. 


## Feature Selection Using Optimization Formulation

It involves formulating the feature selection problem as an optimization problem with a specific objective function and constraints.

The goal is to find the optimal subset of features that maximizes the objective function while satisfying the given constraints. 
  
Feature selection through optimization follows these steps:
1. **Define Objective:** Create a function to measure feature subset quality (e.g., accuracy, information gain).
2. **Set Constraints:** Impose limits (e.g., feature count, correlation) ensuring chosen features meet requirements.
3. **Formulate Problem:** Express feature selection as an optimization task using binary variables for feature selection.
4. **Choose Algorithm:** Select optimization method (e.g., linear/quadratic programming, evolution) based on problem.
5. **Solve Problem:** Apply chosen algorithm to explore feature space, evaluate function, and satisfy constraints.
6. **Evaluate Features:** Use selected features for tasks (e.g., model training), assessing their performance.
7. **Refine and Iterate:** Process is iterative; adjust formulation, constraints, and rerun optimization until satisfactory outcome.

In essence, optimization-driven feature selection involves setting objectives, constraints, and using algorithms to efficiently pick valuable features for tasks like model building.


## Feature Ranking using F-Score

We calculate the F score for each feature independently. What is F-score? It is method to assess the relevance of each feature in datasets based on their individual contribution.

How to do it?
1. Temporarily remove the feature from dataset.
2. Retrain the model using the modified dataset without that feature.
3. Evaluate models performance on testing data , again computing the relevant evaluation metrics.
4. Compute the F-Score of the feature using precision and recall values derived from the evaluation metrics.
5. Repeat steps 1-4 for all features in dataset.

Now rank the features based on their computed f score. Higher F score are considered more relevant. 


## Ensemble Feature ranking
Combining results of multiple ML models we rank features. 
1) Ensemble Model: Each model is trained on dataset using different base models. Like we can use, SVM, decision trees as base models.
2) Feature importance is calculated by seeing how much the models performance deteriorates when feature is removed from the dataset.
3) Once feature importance values are calculate for each model. We can aggregate scores by taking max or avg importance score for each feature.


## Feature ranking and Linear SVM

Linear Regression can be used to rank the importance of features based on their coefficients and weights.

1) Prepare the dataset by separating it into Features(x) and targets(y). Ensure proper scaling.
2) Train it on a SVM model of linear type. Find the optimal hyperplane. Here the model learns weights associated with each feature.
3) Feature Importance Calculation: Extract features, the magnitude of weights are assessed. The positive ones positively contribute to the prediction. While negative do the opposite.
4) Ranking the features. If a feature has greater positive weights then it is of greater importance. 