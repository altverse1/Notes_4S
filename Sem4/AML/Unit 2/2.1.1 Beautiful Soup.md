Python library for web scraping. Provides API to navigate through HTML and XML documents and get data. Does this via creating a parse tree, which allows for easier location of desired elements.


Importing is the first step: 
```python
from bs4 import BeautifulSoup
```

Getting the HTML content is next. We can manually give the HTML content or can get it from the site itself by passing the URL to libraries like requests or urlib.

```python
html_content = """ 
<html> 
	<head> 
		<title>Example.com</title> 
	</head> 
	<body> 
		<p class="paragraph">This is a paragraph.</p> 
		<ul> 
			<li>Item 1</li> 
			<li>Item 2</li> 
		</ul> 
	</body> 
</html> """
```
Then we create a parse tree by creating a BS object and passing the html content along with a parser of choice. (html.parser(default), lxml or html5lib)

```python
soup = BeautifulSoup(html_content, "html.parser") 
title = soup.title #Output with the tags 
title = soup.title.text #Output Content of the tag

para = soup.p
print(para.text)
```


## Going around the parse tree

### Tag Navigation
We can access elements based on their tag names. We use find() method.  
ex: `para = soup.find('p')`

We can also do find_all()
ex: `para = soup.find_all('p')` We can then iterate and print all.


### Parent Sibling Navigation
BS allows access to parent and previous and next siblings.
Ex:
```python
parent_element = element.parent
prev_sibling = element.prev_sibling
next_sibling = element.next_sibling
```


### Child Navigation
We can also do child navigation.
```python
for child in element.children:
	print(child.text)
```


### Searching CSS class and attributes
Allows us to search elements based on their CSS classes or attributes. Ex:
`elements = soup.find_all('div',class_='my-class')`



## Find() Method

It is used to search for the first occurrence of an element that matches the criteria within the parse tree. Allows us to locate elements based on the following parameters.

It's syntax is: `find(name, attrs, recursive, string, **kwargs)`

Here is what each parameters do:
- name: The parameter specifies the tag name to search for. Ex: name = 'div' will search for div tag.
- attrs: Will allow to search for element based on their attributes. Like by passing a dict of attribute value pairs like: `attrs={'class': 'my-class', 'id': 'my-id'}`. Searches for first element with these specs.
- Recursive: controls wether the search is performed recursively(TRUE(DEFAULT)) or only on direct children(FALSE).
- string: searches for string containing specific text. ex: `soup.find(string='example')`. First instance of text 'example'.
- `**kwargs`: additional keyword args that can be passed to search for elements based on any attribute value pair. Ex: `soup.find(data='value')`

Find() returns the first matching element only. If not found it will return None


## Find All

It is used to search for the all occurrence of elements that matches the criteria within the parse tree. Allows us to locate elements based on the following parameters.

It's syntax is: `find_all(name, attrs, recursive, string, limit, **kwargs)`

Here is what each parameters do:
- name: The parameter specifies the tag name to search for. Ex: name = 'div' will search for all div tags.
- attrs: Will allow to search for element based on their attributes. Like by passing a dict of attribute value pairs like: `attrs={'class': 'my-class', 'id': 'my-id'}`. Searches for all element with these specs.
- Recursive: controls wether the search is performed recursively(TRUE(DEFAULT)) or only on direct children(FALSE).
- string: searches for string containing specific text. ex: `soup.find(string='example')`. All instance of text 'example'.
- Limit: Using this parameter we can limit the amount of occurrences of each search to a set number of times.
- `**kwargs`: additional keyword args that can be passed to search for elements based on any attribute value pair. Ex: `soup.find(data='value')`

>[!question]
>Write code to illustrate `find()` and `find_all()` method by taking example of html content with 2 paragraph with a word example one of them
>- Fetch 'p'. Only has the word example first.
>- Give limit = 1 for find_all.

Answer: (It doesn't work in callouts): 
```python
from bs4 import BeautifulSoup
html_content = '''
<html>
<head>
<title> HTML </title>
</head>
<body>
<p> This is an example question </p>
<p> This paragraph dosen't contain that word </p>
</body>
</html>
'''
soup = BeautifulSoup(html_content,'html.parser')
soup.find('p',string="example")
soup.find_all('p',limit=1)
```


## Beautiful Soup Objects (sort of summary)

BS objects represent the parsed HTML/XML document and provide method and attributes for navigation, searching and manipulation. When creating a BS object you pass the HTML/XML to a parser that create the parse tree. Methods like find(), find_all() are used to navigate the parser tree.

- Tag: Represents HTML tag within doc. Can access via dot notation or by find methods.
	```python
	title_tag = soup.title
	paragraph_tag = soup.find_all('p')
	```
- Text: Represents the text inside a tag. Ex: `para_text = soup.p.text`
- `find(name, attrs, recursive, string, **kwargs)`: searches for first occurrence of an element that matches with the criteria and returns a tag object.
- `find_all(name, attrs, recursive, string, limit, **kwargs)`: searches for all occurrence of an element that matches with the criteria and returns list like collection of  tag objects.
- parent: represent's the parent element of a tag. `parent = tag.parent`
- siblings: represent next and the previous siblings of a tag. 
	```python
	next_s = tag.next_sibling
	prev_s = tag.previous_sibling 
	```
- children: provides an iterator over the direct children of a tag
	```python
	for child in tag.children:
		print(child)
	```
- prettify(): returns a neatly formatted representation of document. `formatted = soup.prettify()`
- get: it is used to get value of certain attribute of tag. 
	ex:
	```python
	links = bs.find_all('a')
	for link in links:
	    print(link.get('href'))
	```

## RegEx