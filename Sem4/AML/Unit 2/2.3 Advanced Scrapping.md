
## Creating a simple scrapper

Scrapy is an opensource tool. 
To install:
```
conda install -c conda-forge scrapy
or
pip install scrapy
```


To Start a scrapy project(spider):
`scrapy startproject wikispider`

The Project Tree will looks something like this: 
```
scrapy1
│   scrapy.cfg
│
└───scrapy1
	│   items.py
	│   middlewares.py
	│   pipelines.py
	│   settings.py
	│   __init__.py
	│
	└───spiders
			__init__.py
```


We Create a new crawler under scrapy1/scrapy1/spiders/xyz.py

Consider this example:
```python
import scrapy
class ArticleSpider(scrapy.Spider):
	name = 'article'
	def start_requests(self):
		urls = [
		' http:/en.wikipedia.org/wiki/Python_''%28programming_language%29',
		' https:/en.wikipedia.org/wiki/Functional_programning ' ,
		'https://en.wikipedia.org/wiki/Monty_Python']
		return [scrapy.Request(url=url callback =self.parse) for url in urls]
	
	def parse(self, response):
		url = response.url
		title = response.css( 'hl::text').extract_first()
		print('URL is:{} ', format(url))
		print('Title is: {}', format(title))
```

The spider class is named 'ArticleSpider' which has the spider named `article`
The start_request is a method to generate the initial requests to be sent to the target URLs.
The parse method is executed response of each request is received. 
In the parse method we extract the URL and the Title.

To execute the above we type `scrapy runspider article.py


## Spidering With Rules

Aka rule-based crawling. 
Aim is to crawl the website based on rules and patterns for navigation and extracting data from websites. 
Systematic and structured way of crawling.

```python
import scrapy 
from scrapy.linkextractors import LinkExtractor
from scrpay.spiders import CrawlSpider,Rule

class MySpider(CrawlSpider):
		name = 'example'
		allowed_domains = ['example.com']
		start_url = ['https://www.example.com']
		rules = (
				Rule(LinkExtractor(allow=r'category\.php'),callback = 'parse_category',follow=True),
    )

		def parse_category(self,response):
				item_links = response.css('.item-link::attr(href)').getall()
				for item_link in item_links:
						yield response.follow(item_link,callback=self.parse_item)

		def parse_item(self,response):
				item = {
						'title':response.css('h1::text').get(),
						'price': response.css('.price::text').get(),
						 #other itme data extraction.
				}
```

Class MySpider extends CrawlSpider. 
We crawl example.com.
The rule object defines a pattern for following links and specifies a callback function to handle the response obtained from following those links. Here we instruct spider to follow links on the category page.
The parse_category extracts links from the category page and navigates through it. The parse_item then extracts data from the page, which gets stored into a dictionary. 


## Creating Items

It is the process of defining a structure that allows for storage of scrapped information.

Steps:
1) Create a class that inherits `scrapy.Item`
2) Give a name to the attribute that is extracted, then define them via `scrapy.Field()`

```python
import scrapy
class MyItem(scrapy.Item):
	title = scrapy.Field()
	description = scrapy.Field()
	price = scrapy.Field()
```

3) In the spider instantiate the class and populated the field with scraped data. 
4) Create an instance inside the parse method(ideally) and assign values to it's fields.
5) Yield the populated item from the callback method. This tells scrapy to process and store the scraped item.

```python
import scrapy
class MySpider(scrapy.Spider):
	name='example'
	start_url = ['example.com']
	
	def parse(self.response):
		#Extract data from response
		title = response.css('h1::text').get()
		description = response.css('.description::text').get()
		price = response.css('.price::text').get()
		
		# Create an instance of item class and populate its fields
		items = myItem()
		items['title'] = title
		items['description'] = description
		items['price'] = price

		yield items
```

## Outputting

Scrapy uses the item objects to determine the information it should save from the pages it visits. This info can be saved in JSON, CSV and XML formats.

```shell
scrapy runspider articleItems.py -o articles.csv -t csv
scrapy runspider articleItems.py -o articles.json -t json
scrapy runspider articleItems.py -o articles.xml -t xml
```


## Item Pipeline

It's a component that allows us to define a series of data processing steps for the scraped item before they are stored or further processed. Allows us to clean validate and store the extracted data. It is sequentially executed. 

Working Of Item Pipeline:
1) Config:
	- Modify ITEM_PIPELINES setting under settings.py. 
	- This is a dict. that allows for defining pipelines and their order of execution. Each pipeline is named uniquely. 
```python
ITEM_PIPELINES = { 
"myproject.pipelines.MyPipeline1":100,
"myproject.pipelines.MyPipeline2":200,
"myproject.pipelines.MyPipeline3":300, 
}	
```

2) Components: 
	It has a series of components/pipeline stages that process the items. Each components does a task on the item.
	Custom pipeline can be created by using methods like process_item.
```python
class MyPipeline1(object):

	def process_item(self, item, spider):
		#Processing the items
		#Modify,validate,clean store or drop items
		if item['price'] > 100:
			return item
		else
			return DropItem('Item price is too low')
```

3) Process_item method:
	Its a method of pipeline that receives scrapped item as input on which it performs some operation. We can manipulate, validate, clean, discard irrelevant items from the scrapped data.

  4) Yielding Items:
	  Post processing, we can yield the modified item or drop it. Upon yielding it will move to the next pipeline component.
	  Dropping it means it will be discarded and further pipeline components will not process it.  



## Document Encoding, Text Encoding and Global Internet 

1. **Document Encoding**:
    - Document encoding refers to the character encoding scheme used to represent and interpret text in a document.
    - Scrapy examines the HTTP headers of a web page to determine its document encoding.
    - The encoding information is used to decode the raw HTML response into Unicode text for further processing.
    
2. **Text Encoding**:
    - Text encoding (character encoding) is the process of converting characters into a specific binary representation.
    - It assigns unique numerical values (code points) to characters for electronic storage and transmission.
    - Scrapy ensures that the extracted text from web pages is correctly encoded into Unicode for handling different languages and character sets.
    
3. **Global Internet in Scrapy**:
    - Scrapy operates within the global internet environment, enabling web scraping across various websites and online resources.
    - It uses the HTTP protocol to send requests, receive responses, and navigate websites.
    - Scrapy can extract data from HTML, XML, JSON, and other structured formats.
    - It handles cookies, user sessions, redirects, and respects robots.txt rules.
    - Scrapy supports authentication mechanisms and provides tools to handle web scraping challenges like pagination, AJAX requests, and customization through middleware.



## Using API 
  
Crawling through APIs involves using HTTP methods (GET, POST, PUT, DELETE) to interact with web APIs and retrieve data programmatically.
Web APIs provide a structured way for software applications to communicate and exchange data.
HTTP methods determine the actions to be performed on the server's resources.

1. **Understanding APIs**:
    - APIs define rules and protocols for software applications to interact with each other.
    - Web APIs use HTTP as the communication protocol and provide endpoints (URLs) that respond to specific HTTP requests.

2. **Choosing an API**: 
    - Determine the API you want to crawl, whether it's a public API provided by a third-party service or an API developed by the website itself.

3. **HTTP Methods**: 
    - GET: Used to retrieve data from the server.
    - POST: Used to submit data to the server, often for creating new resources.
    - PUT: Used to update data on the server, typically for modifying existing resources.
    - DELETE: Used to delete data from the server, removing resources.

4. **Sending API Requests**: 
    - Use a programming language or tools like Scrapy, Python's requests library, or Curl to send HTTP requests to the API endpoints.
    - Construct the URL for the API endpoint, including any required query parameters or authentication tokens.
    - Specify the appropriate HTTP method (GET, POST, PUT, DELETE) for the desired action.
    - Send the request to the API endpoint.

5. **Handling API Responses**:
    - Receive the API response, which includes a status code, headers, and the response body (the requested data).
    - Parse and process the response body according to the specific API and your requirements.
    - The response can be in various formats, such as JSON, XML, or plain text.
    - Extract the relevant data from the response body and store or further process it as needed.


Example Program on Webscrapping Using API:

```python
import requests
from datetime import datetime, timedelta

def get_weather_data(api_key, city):
    end_date = datetime.now().strftime("%Y-%m-%d")
    start_date = (datetime.now() - timedelta(days=10)).strftime("%Y-%m-%d")
    
    base_url = f"http://api.weatherapi.com/v1/history.json?key={api_key}&q={city}&dt={start_date}&end_dt={end_date}"
    response = requests.get(base_url)
    data = response.json()
    
    if "error" in data:
        print(f"Error occured: {data['error']['message']}")
    else:
        forecast_days = data["forecast"]["forecastday"]
        for day in forecast_days:
            date = day["date"]
            temperature = day["day"]["avgtemp_c"]
            humidity = day["day"]["avghumidity"]
            weather_description = day["day"]["condition"]["text"]
            
            #Print the weather inf for each day
            print(f"Weather in {city} on {date}:")
            print(f"Temperature: {temperature}°C")
            print(f"Humidity: {humidity}%")
            print(f"Weather Description: {weather_description}")
            print("--------------------")

api_key="YOUR_API_KEY"
city='Karkala'
get_weather_data(api_key, city)
```


## Parsing JSON

JSON format can be easily read and written by parsing it.
Parsing → Convert JSON files to native objects. (Extracting the data)
We will be using the JSON module, to parse the json file.
We use json.loads to parse the json data into a python object such as list or usually a dictionary.
json_data →json file

`data = json.loads(json_data)`

Access it using:

```python
name = data[’name’]
age = data [’age’]
city = data [’city’]
```



## Pillow Library

It is a python imagining library that provides tools for image processing.

To get started we install it via `pip install Pillow`

We can access various methods that allow us to see the image properties, perform image manipulation. 
In this explain the functions.
An Example program:

```python
from PIL import Image

image = Image.open(r'79011148_0.jpg')

#Information about image
print('Image Format:',image.format)
print('Image Mode:',image.mode)
print('Image Size:',image.size)

#Converts image to gray scale
gray_img = image.convert('L')
resized = image.resize((500,500))
rotated = resized.rotate(45)
rotated.save('rotated.jpg')
```


## Scraping Text From Images
Extracting text content embedded in images.
Optical Character Recognition is used.

We need pytesseract(for ocr) and pillow libraries. These can be installed by:
```shell
pip install pytesseract
pip install Pillow
```

The basic program:
```python
import pytesseract
from Pillow import Image
img_url = "https://example.com/image.jpg"

image = Image.open(requests.get(img_url).raw)
# Preprocess image if needed

text_extracted = pytesseract.image_to_string(image)
```

## Reading Captchas
CAPTCHAs are Implimented on websites to prevent automated scraping and that the site is being used only by humans. This is one of the roadblocks to scrapping. 

To handle a CAPTCHA:
- Manual Intervention: Humans can solve the captcha to proceed with the scraping process. Due to the manual intervention it is often slow,
- Captcha solving services: Paid third party services allow human workers, advanced algorithms to bypass captcha instead of manually doing. 
- Captcha bypass technique: We can create automated techniques. Like image recognition algorithms or machine learning approaches. 
These techniques can be applied once their legal implications can be thoroughly analyzed. 


## Training Tesseract in the Scrapping Process

Tesseract is an OCR engine. Allows for extracting text(CAPTCHAs too). Pretrained models in tesseract are not great for this. But we can train on custom CAPTCHA data to improve the accuracy. 

1. Collecting CAPTCHA Training Data:
    - Gather a dataset of CAPTCHA images that represent the variations and challenges you want to overcome.
    - Include the corresponding ground truth text for each CAPTCHA image.

2. Labeling and Preprocessing:
    - Manually transcribe the text in each CAPTCHA image to provide the ground truth.
    - Preprocess the CAPTCHA images, removing noise or distortion to enhance the training process.

3. Training Tesseract:
    - Use Tesseract's training tools to train the OCR model.
    - Provide the labeled CAPTCHA images and their ground truth data.
    - Tesseract learns from this data and adjusts its internal models to improve text recognition and extraction from similar CAPTCHA images.

4. Considerations:
    - Training Tesseract for CAPTCHA recognition can be challenging due to deliberate obfuscation and variations in CAPTCHA designs.
    - Success depends on the complexity of the CAPTCHAs and the quantity and quality of available training data.