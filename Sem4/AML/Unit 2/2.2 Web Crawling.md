Aka Web spider is an automated script that is used to systematically browse and index the web.

Uses of web crawlers:
- **Search engine indexing**, discover webpages, their content and index them
- **Data mining and research**, collect specific data about objects for research
- **Monitoring and Tracking**, price changes etc.
- **Content Aggregation**, gather content for summarization purpose
- **Link Verification**, check for availability of weblinks 

## Web Crawling process

- **Seed URL**: Starts with initial URL, manually provided or from a predefined list or in a DB.
- **HTTP Request**: Request to web server asking for the webpage.
- **Web Page Retrieval**: HTML of the webpage is got by the crawler.
- **Parsing HTML**: Crawler parses through the HTML to get links, images links, keywords etc.
- **URL Frontier:** Queue of URLs to visit, adds newly found URLs for crawling .
- **Follow Link**: Follows found links and adds more URLs for crawling.
- **Throttling and Politeness**: Throttles or enforcing rate limits to avoid server overload.
- **Data Storage**: Storing the data in a format like JSON etc. for further processing.



## Crawling Policies and Challenges

- **Robotx.txt**: crawlers respect this file, it declares what content can and cannot be crawled.
- **Duplicate Content**: URL normalization to avoid duplicate content. Avoids going through the same URL over and over again.
- **Crawl Scope**: Define the scope and depth of crawl, limitations on domains and subdomains etc.
- **Dynamic Content**: Crawlers struggle with dynamic websites/JS heavy websites, may require user interaction to access pages.
- **Anti Crawling Measures**: Anti crawling measures like Captchas, IP blocking or session tracker to block crawling. 

Example of Crawler:

```python
import requests
from bs4 import BeautifulSoup

def crawl_web(url):
	response = requests.get(url)
	if response.status_code = 200:
		soup = BeautifulSoup(response.content, 'html.parser')
		links = soup.find_all('a') #Extract all links
		for link in links:
			href = link.get('href')
			print(href)

url = 'https://www.example.com'
crawl_web(url)
```


## Web Crawler: To search an entire site

Systematically navigate through pages to search for content. Visits multiple page for desired data.

### Steps involved are as follows

- **Starting Point**: Select specific page from which the crawling process will start. Ex: Homepage. 
- **GET Request**: Uses Python library like Requests to send a request to retrieve the HTML content and receive a response.
- **Parse HTML** : Use an HTML parser to navigate through the HTML structure.
- **Search for Element**: Search for specific tags like links, paragraphs; images, keywords etc. Use regex if required.
- **Extract information**: If data is found, extract it along with it's metadata, associated links etc.
- **Follow Links**: Follow the newly found links in the page via a queue. Ensure that no loop occurs. 

```python
import requests
from bs4 import BeautifulSoup
from urlib.parse import urljoin

def crawl_web(url, search_keyword):
	visited = set()
	queue = [url]
	
	while queue:
		pg_url = queue.pop(0)
		if pg_url in visited:
			continue
		response = requests.get(pg_url)
		if response.status_code = 200:
			soup = BeautifulSoup(response.content, 'html.parser')
			if search_keyword in soup.get_text():
				print("Keyword found at: ",pg_url)
			links = soup.find_all('a') #Extract all links
			for link in links:
				href = link.get('href')
				abs_url = urljoin(page_url,href)
				queue.append(abs_url)
		visited.add(pg_url)

url = 'https://www.example.com'
search_keyword = 'example'
crawl_web(url, search_keyword)
```


## Web Crawling for Internet

An enormous task. Involves getting external links during crawling process. 

Steps to get ext. links:
- **Seed URLs**: Select initial URLs of the domain to crawl. Ex: Homepage.
- **Crawling and Parsing**: Get the html content by sending a request. Parse the HTML by using tools like BS to get relevant info.
- **Link Extraction**: Identify and extract the links from the parsed content. By searching `<a>` tags and then getting the `href` attributes which are the URL of the link.
- **Filtering Internal & External URL**:
	1) Internal: URLs that refer to the same domain.
	2) External: URLs that refer to the domain outside the domain that is being crawled.
- **Handling the external links**: Handle them via these options:
	1) Ignore: Ignore external links. Focuses only on current domain.
	2) Follow: Allows to go to external domains to expand on information gathering.
	3) Record: Keep a record of external links for further analysis.
- **Recursive Crawling**: Recursive crawling allows for expanding reach across the internet by following external links.
	![[aml_s4.excalidraw#^group=O6FM6qw85zoeRRE2ynRZr]]
## Defining Objects

Defining the data to collect based solely on what is visible on a website can lead to an unsustainable and inefficient approach to web scraping. Adding fields every time a new piece of information is found on a website can result in a large and unmanageable number of fields.

- A more sustainable approach is to define a standardized set of fields that are relevant to the domain or data you want to collect.
- Rather than adding new fields for each website, focus on extracting the most important and relevant data that surely appears across different websites.

Ex: For clothing items some site has attributes: Name, Color, Size, Brand. While site 2 might have these attributes along with others like, Brand Description, Brand first appearance, Stock, etc.
Final set of attributes depends a lot on what this data will be used for and how consistent will it be across different sites.


## Dealing with different website layouts

If we are dealing with different layouts we can predefine different crawlers inside functions

```python
if response.status_code==200:
	soup = BeautifulSoup(response.content, 'html.parser')
	if is_layout_type_a(url):
		extract_type_a(soup)
	elif is_layout_type_b(url):
		extract_type_b(soup)
	else:
		extract_default(soup)

def is_layout_type_a(url):
	# logic to check type a
	return
def extract_type_a(soup):
	# logic to extract type a
	return
def is_layout_type_b(url):
	# logic to check type b
	return
def extract_type_b(soup):
	# logic to extract type b
	return
```

>Links
	[[2.3 Advanced Scrapping]]